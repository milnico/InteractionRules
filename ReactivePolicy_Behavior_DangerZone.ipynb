{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milnico/InteractionRules/blob/main/ReactivePolicy_Behavior_DangerZone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIC7uUDNOY5w"
      },
      "source": [
        "## Pre-requisite\n",
        "\n",
        "Before we start, we need to install EvoJAX and import some libraries.  \n",
        "**Note** In our [paper](https://arxiv.org/abs/2202.05008), we ran the experiments on NVIDIA V100 GPU(s). Your results can be different from ours.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r32kSDNZaV4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7de884-2b04-4a98-9780-722a665e9fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evojax\n",
            "  Downloading evojax-0.2.15-py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.1/94.1 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evojax\n",
            "Successfully installed evojax-0.2.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flax==0.6.0\n",
            "  Downloading flax-0.6.0-py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.1/180.1 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flax\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.6.7\n",
            "    Uninstalling flax-0.6.7:\n",
            "      Successfully uninstalled flax-0.6.7\n",
            "Successfully installed flax-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rich==11.2.0\n",
            "  Downloading rich-11.2.0-py3-none-any.whl (217 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.3/217.3 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rich\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.3.3\n",
            "    Uninstalling rich-13.3.3:\n",
            "      Successfully uninstalled rich-13.3.3\n",
            "Successfully installed rich-11.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cma==3.2.2\n",
            "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from cma==3.2.2) (1.22.4)\n",
            "Installing collected packages: cma\n",
            "Successfully installed cma-3.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax==0.1.3\n",
            "  Downloading optax-0.1.3-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.1/145.1 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optax\n",
            "  Attempting uninstall: optax\n",
            "    Found existing installation: optax 0.1.4\n",
            "    Uninstalling optax-0.1.4:\n",
            "      Successfully uninstalled optax-0.1.4\n",
            "Successfully installed optax-0.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting commonmark==0.9.1\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: commonmark\n",
            "Successfully installed commonmark-0.9.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chex==0.1.5\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chex\n",
            "  Attempting uninstall: chex\n",
            "    Found existing installation: chex 0.1.7\n",
            "    Uninstalling chex-0.1.7:\n",
            "      Successfully uninstalled chex-0.1.7\n",
            "Successfully installed chex-0.1.5\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output, Image\n",
        "\n",
        "!pip install evojax --no-dependencies\n",
        "!pip install flax==0.6.0 --no-dependencies\n",
        "!pip install rich==11.2.0 --no-dependencies\n",
        "!pip install cma==3.2.2\n",
        "!pip install optax==0.1.3 --no-dependencies\n",
        "!pip install commonmark==0.9.1 --no-dependencies\n",
        "!pip install chex==0.1.5 --no-dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Testo del titolo predefinito\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VukrNkbJoM43",
        "outputId": "6e98adf1-b03e-49e3-91ba-fadd17b459ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-3d4NLAE8hA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06e2a35-37d9-47ff-d628-7bf7945d6314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.9/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from evojax.task.cartpole import CartPoleSwingUp\n",
        "from evojax.policy.mlp import MLPPolicy\n",
        "from evojax.algo import PGPE\n",
        "from evojax import Trainer\n",
        "from evojax.util import create_logger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2022 The EvoJAX Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Implementation of a multi-agents WaterWorld task.\n",
        "\n",
        "Ref: https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html\n",
        "\"\"\"\n",
        "\n",
        "from typing import Tuple\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import tree_util\n",
        "from flax.struct import dataclass\n",
        "\n",
        "from evojax.task.base import TaskState\n",
        "from evojax.task.base import VectorizedTask\n",
        "\n",
        "\n",
        "SCREEN_W = 1000\n",
        "SCREEN_H = 1000\n",
        "SCALE = 1\n",
        "BUBBLE_RADIUS = 2.5\n",
        "EATING_RADIUS = 50\n",
        "MIN_DIST = 2 * BUBBLE_RADIUS\n",
        "MAX_RANGE = 400\n",
        "NUM_RANGE_SENSORS = 8\n",
        "DELTA_ANG = 3.14 / NUM_RANGE_SENSORS #2 * 3.14 / NUM_RANGE_SENSORS\n",
        "\n",
        "TYPE_VOID = 0\n",
        "TYPE_WALL = 1\n",
        "TYPE_FOOD = 2\n",
        "TYPE_POISON = 3\n",
        "TYPE_AGENT = 4\n",
        "SENSOR_DATA_DIM = 1\n",
        "\n",
        "ACT_UP = 0\n",
        "ACT_DOWN = 1\n",
        "ACT_LEFT = 2\n",
        "ACT_RIGHT = 3\n",
        "SEED = 0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BubbleStatus(object):\n",
        "    pos_x: jnp.float32\n",
        "    pos_y: jnp.float32\n",
        "    vel_x: jnp.float32\n",
        "    vel_y: jnp.float32\n",
        "    bubble_type: jnp.int32\n",
        "    valid: jnp.int32\n",
        "    direction: jnp.float32\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class State(TaskState):\n",
        "    agent_state: BubbleStatus\n",
        "    item_state: BubbleStatus\n",
        "    obs: jnp.ndarray\n",
        "    steps: jnp.int32\n",
        "    key: jnp.ndarray\n",
        "\n",
        "\n",
        "@partial(jax.vmap, in_axes=(0, None))\n",
        "def create_bubbles(key: jnp.ndarray, is_agent: bool) -> BubbleStatus:\n",
        "    center_x = random.uniform(key.val[-1]) * (SCREEN_W - 200) + 100\n",
        "    center_y = random.uniform(key.val[-2]) * (SCREEN_H - 200) + 100\n",
        "    k_pos_r, k_pos_theta, k_vel, k_center_x, k_center_y = random.split(key, 5)\n",
        "\n",
        "    if is_agent:\n",
        "        bubble_type = TYPE_AGENT\n",
        "        #vel_x = random.uniform(k_vel, shape=(), minval=5.0, maxval=5.0)\n",
        "        #vel_y = random.uniform(k_vel, shape=(), minval=5.0, maxval=5.0)\n",
        "        #r = random.uniform(k_pos_r, shape=(), minval=0, maxval=200)\n",
        "        #theta = random.uniform(k_pos_theta, shape=(), minval=0, maxval=2 * jnp.pi)\n",
        "        pos_x =  random.uniform(k_center_x, shape=(), minval=10, maxval=SCREEN_W-10)#center_x + random.normal(k_center_x, shape=()) * 100\n",
        "        pos_y =  random.uniform(k_center_y, shape=(), minval=10, maxval=SCREEN_H-10)#center_y + random.normal(k_center_y, shape=()) * 100\n",
        "        direction = random.uniform(k_pos_theta, shape=(), minval=-3.14, maxval=3.14)\n",
        "        vel_x = jnp.cos(direction)*5.0\n",
        "        vel_y = jnp.sin(direction)*5.0\n",
        "    else:\n",
        "        bubble_type = TYPE_FOOD\n",
        "        vel_x = vel_y = 0.\n",
        "        pos_x = random.uniform(k_pos_r, shape=(),minval=100, maxval=SCREEN_W-100)\n",
        "        pos_y = random.uniform(k_vel, shape=(),minval=100, maxval=SCREEN_H-100)\n",
        "        direction = random.uniform(k_pos_theta, shape=(), minval=-3.14, maxval=3.14)\n",
        "\n",
        "    return BubbleStatus(pos_x=pos_x, pos_y=pos_y, vel_x=vel_x, vel_y=vel_y,\n",
        "                        bubble_type=bubble_type, valid=1, direction=direction)\n",
        "\n",
        "'''\n",
        "def get_item_move(direction_x: jnp.ndarray,direction_y: jnp.ndarray,\n",
        "               items: BubbleStatus) -> Tuple[BubbleStatus,\n",
        "                                             jnp.float32]:\n",
        "    dist = jnp.sqrt(jnp.square(items.pos_x - direction_x) +\n",
        "                    jnp.square(items.pos_y - direction_y))\n",
        "    dist_bin = jnp.where(dist==jnp.min(dist),1,0)\n",
        "    nearest_prey_x = jnp.sum(jnp.multiply(dist_bin,direction_x))\n",
        "    nearest_prey_y = jnp.sum(jnp.multiply(dist_bin, direction_y))\n",
        "\n",
        "\n",
        "    orient = -jnp.arctan2(nearest_prey_y - items.pos_y, nearest_prey_x - items.pos_x)\n",
        "    vel_x = jnp.cos(orient) * 5.0\n",
        "    vel_y = jnp.sin(orient) * 5.0\n",
        "\n",
        "    pos_x = items.pos_x + vel_x\n",
        "    pos_y = items.pos_y - vel_y\n",
        "    # Collide with the west wall.\n",
        "    pos_x = jnp.where(pos_x < 1, pos_x + SCREEN_W - 1, pos_x)\n",
        "    # Collide with the east wall.\n",
        "    pos_x = jnp.where(pos_x > SCREEN_W - 1, pos_x - SCREEN_W + 1, pos_x)\n",
        "    # Collide with the north wall.\n",
        "    pos_y = jnp.where(pos_y < 1, pos_y + SCREEN_H - 1, pos_y)\n",
        "    # Collide with the south wall.\n",
        "    pos_y = jnp.where(pos_y > SCREEN_H - 1, pos_y - SCREEN_H + 1, pos_y)\n",
        "\n",
        "    reward = 0\n",
        "\n",
        "\n",
        "    items_state = BubbleStatus(\n",
        "        pos_x=pos_x, pos_y=pos_y,\n",
        "        vel_x=items.vel_x, vel_y=items.vel_y,\n",
        "        bubble_type=items.bubble_type,\n",
        "        valid=items.valid, direction=orient)\n",
        "    return items_state, reward\n",
        "'''\n",
        "def get_reward(items: BubbleStatus,\n",
        "               agent: BubbleStatus,\n",
        "               key: jnp.ndarray,\n",
        "               agent_distance) -> Tuple[BubbleStatus,\n",
        "                                             BubbleStatus,\n",
        "                                             jnp.float32]:\n",
        "    #noise_dist,pp = random.split(key)\n",
        "    predator_visual = jnp.pi*(3/4)\n",
        "    old_dir_pred = items.direction\n",
        "    dist = jnp.sqrt(jnp.square(items.pos_x - agent.pos_x) +\n",
        "                    jnp.square(items.pos_y - agent.pos_y))*agent.valid\n",
        "\n",
        "    #angles between the predator and the preys\n",
        "    angles = -jnp.arctan2(agent.pos_y - items.pos_y, agent.pos_x - items.pos_x)\n",
        "    angles = (angles - old_dir_pred).ravel()\n",
        "    angles = jnp.where(angles > jnp.pi, angles - (jnp.pi * 2), angles)\n",
        "    angles = jnp.where(angles < -jnp.pi, angles + (jnp.pi * 2), angles)\n",
        "    angles_bin = jnp.where((angles > -predator_visual) & (angles < predator_visual), 1.0, 0.0)\n",
        "    angles_dist = angles_bin*dist\n",
        "\n",
        "    #angles_dist =  jnp.where(angles_dist>0.0,angles_dist + random.uniform(noise_dist, shape=jnp.shape(angles_dist), minval=-20, maxval=20),0.0)\n",
        "    #agent_valid = (dist >= BUBBLE_RADIUS*2) * items.valid\n",
        "    #dist = jnp.where(dist==0.0,1000,dist)\n",
        "    #angles_dist = jnp.where(angles_dist == 0.0, 1000, angles_dist)\n",
        "    #dist_bin = jnp.where(angles_dist==jnp.min(angles_dist),1,0)\n",
        "\n",
        "\n",
        "    nearest_prey_x = agent.pos_x[jnp.argmin(jnp.where(angles_dist == 0.0, 1000, angles_dist))]#jnp.sum(jnp.multiply(dist_bin,agent.pos_x))\n",
        "    nearest_prey_y = agent.pos_y[jnp.argmin(jnp.where(angles_dist == 0.0, 1000, angles_dist))]#jnp.sum(jnp.multiply(dist_bin, agent.pos_y))\n",
        "\n",
        "\n",
        "    orient = -jnp.arctan2(nearest_prey_y - items.pos_y, nearest_prey_x - items.pos_x)\n",
        "    vel_x = jnp.cos(orient) * 6.0\n",
        "    vel_y = jnp.sin(orient) * 6.0\n",
        "\n",
        "    pos_x = items.pos_x + vel_x\n",
        "    pos_y = items.pos_y - vel_y\n",
        "    # Collide with the west wall.\n",
        "    pos_x = jnp.where(pos_x < 1, pos_x + SCREEN_W - 1, pos_x)\n",
        "    # Collide with the east wall.\n",
        "    pos_x = jnp.where(pos_x > SCREEN_W - 1, pos_x - SCREEN_W + 1, pos_x)\n",
        "    # Collide with the north wall.\n",
        "    pos_y = jnp.where(pos_y < 1, pos_y + SCREEN_H - 1, pos_y)\n",
        "    # Collide with the south wall.\n",
        "    pos_y = jnp.where(pos_y > SCREEN_H - 1, pos_y - SCREEN_H + 1, pos_y)\n",
        "\n",
        "    dist = jnp.sqrt(jnp.square(pos_x - agent.pos_x) +\n",
        "                    jnp.square(pos_y - agent.pos_y)) * agent.valid\n",
        "\n",
        "    eat_prob = jnp.where((dist > 0.0) & (dist <= EATING_RADIUS*2), 1-(dist/(EATING_RADIUS*2)), 0.0)\n",
        "    escape_prob = agent_distance#(1-(agent_distance/jnp.amax(agent_distance)))*0.5\n",
        "    cumulative_prob = jnp.where((dist > 0.0) & (dist <= EATING_RADIUS*2),(eat_prob-2*escape_prob),0.0)\n",
        "    \n",
        "\n",
        "    rnd_gen = random.uniform(key[0], shape=jnp.shape(cumulative_prob), minval=0, maxval=1)#random.uniform(pp, shape=jnp.shape(cumulative_prob), minval=0, maxval=1)\n",
        "    \n",
        "    rnd_process = jnp.where(cumulative_prob==jnp.amax(cumulative_prob),cumulative_prob,0)\n",
        "    eaten = (rnd_process>rnd_gen)\n",
        "    reward = -jnp.sum(eaten)\n",
        "    agent_valid = (1-eaten)*agent.valid\n",
        "    #reward = jnp.any(jnp.where((dist > 0.0) & (dist <= BUBBLE_RADIUS*2), 1.0, 0.0))\n",
        "    #reward = jnp.where(reward==True,-1,0)\n",
        "    #agent_valid = (dist >= BUBBLE_RADIUS * 2)\n",
        "\n",
        "    #dist = jnp.sqrt(jnp.square(agent.pos_x - items.pos_x) +\n",
        "     #               jnp.square(agent.pos_y - items.pos_y))\n",
        "    #rewards = (jnp.where(items.bubble_type == TYPE_FOOD, 1., -1.) *\n",
        "    #           items.valid * jnp.where(dist < MIN_DIST, 1, 0))\n",
        "    #poison_cnt = jnp.sum(jnp.where(rewards == -1., 1, 0)) + agent.poison_cnt\n",
        "    #reward = jnp.sum(rewards)\n",
        "\n",
        "    agent_state = BubbleStatus(\n",
        "        pos_x=agent.pos_x, pos_y=agent.pos_y,\n",
        "        vel_x=agent.vel_x, vel_y=agent.vel_y,\n",
        "        bubble_type=agent.bubble_type,\n",
        "        valid=agent_valid, direction=agent.direction)\n",
        "    items_state = BubbleStatus(\n",
        "        pos_x=pos_x, pos_y=pos_y,\n",
        "        vel_x=items.vel_x, vel_y=items.vel_y,\n",
        "        bubble_type=items.bubble_type,\n",
        "        valid=items.valid, direction=orient)\n",
        "    return agent_state, items_state, reward\n",
        "\n",
        "\n",
        "@partial(jax.vmap, in_axes=(0, None,None,None))\n",
        "def get_rewards(items: BubbleStatus,\n",
        "                agents: BubbleStatus,\n",
        "                key: jnp.ndarray,\n",
        "                agent_distance: jnp.array) -> Tuple[BubbleStatus,\n",
        "                                              BubbleStatus,\n",
        "                                              jnp.ndarray]:\n",
        "    return get_reward(items, agents,key,agent_distance)\n",
        "\n",
        "\n",
        "\n",
        "@jax.vmap\n",
        "def update_item_state(item: BubbleStatus) -> BubbleStatus:\n",
        "    '''\n",
        "    vel_x = item.vel_x\n",
        "    vel_y = item.vel_y\n",
        "    pos_x = item.pos_x + vel_x\n",
        "    pos_y = item.pos_y + vel_y\n",
        "    # Collide with the west wall.\n",
        "    vel_x = jnp.where(pos_x < 1, -vel_x, vel_x)\n",
        "    pos_x = jnp.where(pos_x < 1, 1, pos_x)\n",
        "    # Collide with the east wall.\n",
        "    vel_x = jnp.where(pos_x > SCREEN_W - 1, -vel_x, vel_x)\n",
        "    pos_x = jnp.where(pos_x > SCREEN_W - 1, SCREEN_W - 1, pos_x)\n",
        "    # Collide with the north wall.\n",
        "    vel_y = jnp.where(pos_y < 1, -vel_y, vel_y)\n",
        "    pos_y = jnp.where(pos_y < 1, 1, pos_y)\n",
        "    # Collide with the south wall.\n",
        "    vel_y = jnp.where(pos_y > SCREEN_H - 1, -vel_y, vel_y)\n",
        "    pos_y = jnp.where(pos_y > SCREEN_H - 1, SCREEN_H - 1, pos_y)\n",
        "    '''\n",
        "    return BubbleStatus(\n",
        "        pos_x=item.pos_x, pos_y=item.pos_y, vel_x=item.vel_x, vel_y=item.vel_y,\n",
        "        bubble_type=item.bubble_type, valid=item.valid,\n",
        "        direction=item.direction)\n",
        "\n",
        "@jax.vmap\n",
        "def update_agent_state(agent: BubbleStatus,\n",
        "                       action: jnp.ndarray,key: jnp.ndarray) -> BubbleStatus:\n",
        "    old_dir = agent.direction\n",
        "    k_noise_x, k_noise_y, k_noise_px, k_noise_py = random.split(key, 4)\n",
        "    '''\n",
        "    vel_x = agent.vel_x\n",
        "    vel_y = agent.vel_y\n",
        "    w = (1/(1 + jnp.exp(-weights)))*10.0\n",
        "    flock_dir = direction[:NUM_RANGE_SENSORS-1]\n",
        "    w_f = w[NUM_RANGE_SENSORS-1]\n",
        "\n",
        "    # swarm flocking part\n",
        "    k_noise_x, k_noise_y, k_noise_px, k_noise_py = random.split(key, 4)\n",
        "    divisor = jnp.sum(jnp.where(flock_dir != 0.0, 1, 0)*w_f)\n",
        "    divisor = jnp.where(divisor == 0.0, 1.0, divisor)\n",
        "    # circular mean of the angles\n",
        "    y_comp = jnp.sum(jnp.sin(flock_dir)*w_f) / divisor\n",
        "    x_comp = jnp.sum(jnp.where(flock_dir == 0.0, 0.0, jnp.cos(flock_dir))*w_f) / divisor\n",
        "    av_angle = jnp.arctan2(y_comp, x_comp)\n",
        "    orient_flock = av_angle\n",
        "    #orient = jnp.where(orient_flock==0.0,old_dir,jnp.arctan2((jnp.sin(orient_flock)+jnp.sin(old_dir))/2,(jnp.cos(orient_flock)+jnp.cos(old_dir))/2))\n",
        "\n",
        "    #predator part\n",
        "    predator_dir = jnp.sum(direction[NUM_RANGE_SENSORS - 1:])\n",
        "    angle_to_predator = jnp.where(predator_dir == 0, 0.0, predator_dir - jnp.pi)\n",
        "    angle_to_predator = jnp.where(angle_to_predator > jnp.pi, angle_to_predator - (jnp.pi * 2), angle_to_predator)\n",
        "    angle_to_predator = jnp.where(angle_to_predator < -jnp.pi, angle_to_predator + (jnp.pi * 2), angle_to_predator)\n",
        "    #angle_to_predator= angle_to_predator - jnp.pi*jnp.sign(angle_to_predator)\n",
        "\n",
        "    orient = jnp.where(angle_to_predator == 0.0, orient_flock,\n",
        "                       jnp.arctan2((jnp.sin(orient_flock)*w[-3] + jnp.sin(angle_to_predator)*w[-2] + jnp.sin(old_dir)*w[-1]) / (w[-3]+w[-2]+w[-1]),\n",
        "                                   (jnp.cos(orient_flock)*w[-3] + jnp.cos(angle_to_predator)*w[-2] + jnp.cos(old_dir)*w[-1]) / (w[-3]+w[-2]+w[-1])))\n",
        "    orient += random.uniform(k_noise_y, shape=(), minval=-3.14, maxval=3.14) * 0.1\n",
        "    '''\n",
        "    orient = (old_dir + jnp.pi) - jnp.clip(action[0],-1,1)\n",
        "    orient = jnp.where(orient > jnp.pi * 2, orient - (jnp.pi * 2),orient)\n",
        "    orient = jnp.where(orient < 0.0, orient + (jnp.pi * 2), orient)\n",
        "    orient -= jnp.pi\n",
        "    orient += random.uniform(k_noise_x, shape=(), minval=-3.14, maxval=3.14) * 0.1\n",
        "    vel_x = jnp.cos(orient) * 5.0\n",
        "    vel_y = jnp.sin(orient) * 5.0\n",
        "    pos_x = agent.pos_x + vel_x\n",
        "    pos_y = agent.pos_y - vel_y\n",
        "    orient = -jnp.arctan2(pos_y - agent.pos_y, pos_x - agent.pos_x)\n",
        "    # Collide with the west wall.\n",
        "    pos_x = jnp.where(pos_x < 1, pos_x + SCREEN_W - 1, pos_x)\n",
        "    # Collide with the east wall.\n",
        "    pos_x = jnp.where(pos_x > SCREEN_W - 1, pos_x - SCREEN_W + 1, pos_x)\n",
        "    # Collide with the north wall.\n",
        "    pos_y = jnp.where(pos_y < 1, pos_y + SCREEN_H - 1, pos_y)\n",
        "    # Collide with the south wall.\n",
        "    pos_y = jnp.where(pos_y > SCREEN_H - 1, pos_y - SCREEN_H + 1, pos_y)\n",
        "\n",
        "\n",
        "\n",
        "    return BubbleStatus(\n",
        "        pos_x=pos_x, pos_y=pos_y, vel_x=vel_x, vel_y=vel_y,\n",
        "        bubble_type=agent.bubble_type, valid=agent.valid,\n",
        "        direction=orient)\n",
        "\n",
        "\n",
        "@jax.vmap\n",
        "def get_line_seg_intersection(x1: jnp.float32,\n",
        "                              y1: jnp.float32,\n",
        "                              x2: jnp.float32,\n",
        "                              y2: jnp.float32,\n",
        "                              x3: jnp.float32,\n",
        "                              y3: jnp.float32,\n",
        "                              x4: jnp.float32,\n",
        "                              y4: jnp.float32) -> Tuple[np.bool, jnp.ndarray]:\n",
        "    \"\"\"Determine if line segment (x1, y1, x2, y2) intersects with line\n",
        "    segment (x3, y3, x4, y4), and return the intersection coordinate.\n",
        "    \"\"\"\n",
        "    denominator = (y4 - y3) * (x2 - x1) - (x4 - x3) * (y2 - y1)\n",
        "    ua = jnp.where(\n",
        "        jnp.isclose(denominator, 0.0), 0,\n",
        "        ((x4 - x3) * (y1 - y3) - (y4 - y3) * (x1 - x3)) / denominator)\n",
        "    mask1 = jnp.bitwise_and(ua > 0., ua < 1.)\n",
        "    ub = jnp.where(\n",
        "        jnp.isclose(denominator, 0.0), 0,\n",
        "        ((x2 - x1) * (y1 - y3) - (y2 - y1) * (x1 - x3)) / denominator)\n",
        "    mask2 = jnp.bitwise_and(ub > 0., ub < 1.)\n",
        "    intersected = jnp.bitwise_and(mask1, mask2)\n",
        "    x_intersection = x1 + ua * (x2 - x1)\n",
        "    y_intersection = y1 + ua * (y2 - y1)\n",
        "    up = jnp.where(intersected,\n",
        "                   jnp.array([x_intersection, y_intersection]),\n",
        "                   jnp.array([SCREEN_W, SCREEN_W]))\n",
        "    return intersected, up\n",
        "\n",
        "\n",
        "@jax.vmap\n",
        "def get_line_dot_intersection(x1: jnp.float32,\n",
        "                              y1: jnp.float32,\n",
        "                              x2: jnp.float32,\n",
        "                              y2: jnp.float32,\n",
        "                              x3: jnp.float32,\n",
        "                              y3: jnp.float32) -> Tuple[np.bool, jnp.ndarray]:\n",
        "    \"\"\"Determine if a line segment (x1, y1, x2, y2) intersects with a dot at\n",
        "    (x3, y3) with radius BUBBLE_RADIUS, if so return the point of intersection.\n",
        "    \"\"\"\n",
        "    point_xy = jnp.array([x3, y3])\n",
        "    v = jnp.array([y2 - y1, x1 - x2])\n",
        "    v_len = jnp.linalg.norm(v)\n",
        "    d = jnp.abs((x2 - x1) * (y1 - y3) - (x1 - x3) * (y2 - y1)) / v_len\n",
        "    up = point_xy + v / v_len * d\n",
        "    ua = jnp.where(jnp.abs(x2 - x1) > jnp.abs(y2 - y1),\n",
        "                   (up[0] - x1) / (x2 - x1),\n",
        "                   (up[1] - y1) / (y2 - y1))\n",
        "    ua = jnp.where(d > BUBBLE_RADIUS, 0, ua)\n",
        "    intersected = jnp.bitwise_and(ua > 0., ua < 1.)\n",
        "    return intersected, up\n",
        "\n",
        "\n",
        "@partial(jax.vmap, in_axes=(0, None, None, None))\n",
        "def get_obs(agent: BubbleStatus,\n",
        "            agents: BubbleStatus,\n",
        "            items: BubbleStatus,\n",
        "            walls: jnp.ndarray) -> Tuple[np.float32, jnp.ndarray]:\n",
        "    sensor_obs = []\n",
        "    distance_obs = []\n",
        "    treshold_dist=100\n",
        "\n",
        "    #prev_eating = jnp.array([agent.eat_cnt]).ravel()\n",
        "\n",
        "    agent_xy = jnp.array([agent.pos_x, agent.pos_y]).ravel()\n",
        "   # print(agent_xy)\n",
        "\n",
        "\n",
        "    agent_dir = jnp.array([agent.direction]).ravel()\n",
        "    agents_dir = jnp.array([agents.direction]).ravel()\n",
        "    dist = (jnp.sqrt(jnp.square(agent_xy[0] - agents.pos_x) +\n",
        "                    jnp.square(agent_xy[1] - agents.pos_y))*agents.valid).ravel()\n",
        "    dist_binary = jnp.where((dist > 0.0) & (dist < treshold_dist), 1, 0)\n",
        "    dist_flocking = jnp.where((dist > 0.0) & (dist < treshold_dist/2), 1, 0)\n",
        "    angles = -jnp.arctan2(agents.pos_y - agent_xy[1],agents.pos_x-agent_xy[0])\n",
        "    angles = (angles - agent_dir).ravel()\n",
        "    angles = jnp.where(angles > jnp.pi, angles - (jnp.pi * 2), angles)\n",
        "    angles = jnp.where(angles < -jnp.pi, angles + (jnp.pi * 2), angles)\n",
        "    #predator\n",
        "    dist_pred = (jnp.sqrt(jnp.square(agent_xy[0] - items.pos_x) +\n",
        "                    jnp.square(agent_xy[1] - items.pos_y))).ravel()\n",
        "    dist_pred_binary = jnp.where((dist_pred>0.0)&(dist_pred<treshold_dist*2.0),1,0)\n",
        "    pred_angle = -jnp.arctan2(items.pos_y - agent_xy[1], items.pos_x - agent_xy[0])\n",
        "\n",
        "    predator_obs_angle = jnp.multiply(pred_angle, dist_pred_binary)\n",
        "    #pred_angle = (pred_angle - agent_dir).ravel()\n",
        "    #pred_angle = jnp.where(pred_angle > jnp.pi, pred_angle - (jnp.pi * 2), pred_angle)\n",
        "    #pred_angle = jnp.where(pred_angle < -jnp.pi, pred_angle + (jnp.pi * 2), pred_angle)\n",
        "\n",
        "    step_theta = np.linspace(-jnp.pi*(3 / 4), jnp.pi*( 3/ 4), NUM_RANGE_SENSORS)\n",
        "    #angles = jnp.delete(angles, jnp.where(angles == 0.0))\n",
        "    #print(dist)\n",
        "    #print(angles)\n",
        "    #input(\"getobs\")\n",
        "    #dist = dist[dist!=0]\n",
        "    #angles = angles[angles!=0]\n",
        "    #dirs = agents.direction[]\n",
        "    #comm_array = jnp.argwhere(dist_mat)\n",
        "\n",
        "    #active_comunication = jnp.multiply(dist_mat,agent_comm)\n",
        "    #perceived_angle = jnp.multiply(active_comunication,angles)\n",
        "    #perceived_angle = jnp.sum(perceived_angle)\n",
        "    #angles must be averaged by active comunicators\n",
        "\n",
        "\n",
        "    #comm_activation = jnp.sum(active_comunication)\n",
        "    #divisor = jnp.where(comm_activation==0.0,1.0,comm_activation)\n",
        "\n",
        "    for i in range(NUM_RANGE_SENSORS-1):\n",
        "\n",
        "        v = jnp.where((angles > step_theta[i]) & (angles < step_theta[i + 1]), 1.0, 0.0)\n",
        "        #p = jnp.where((pred_angle > step_theta[i]) & (pred_angle < step_theta[i + 1]), 1.0, 0.0)\n",
        "        d = jnp.multiply(v, dist_binary)\n",
        "        d_flock = jnp.multiply(v, dist_flocking)\n",
        "        #d_p = jnp.multiply(p, dist_pred)\n",
        "        value = jnp.where(jnp.sum(d) == 0.0, 0, agents_dir[jnp.argmin(jnp.where(d == 0.0, 1000, d))])\n",
        "        #value_dist = dist[jnp.argmin(jnp.where(dd == 0.0, 1000, dd))]\n",
        "        value_dist = jnp.where(jnp.sum(d_flock) == 0.0, 0, 1.0)\n",
        "        clockwise_distance = (agent_dir+jnp.pi)-(value+jnp.pi)\n",
        "        clockwise_distance = jnp.where(clockwise_distance > jnp.pi*2, clockwise_distance - (jnp.pi * 2), clockwise_distance)\n",
        "        clockwise_distance = jnp.where(clockwise_distance < 0.0, clockwise_distance + (jnp.pi * 2), clockwise_distance)\n",
        "        anticlockwise_distance = clockwise_distance-jnp.pi*2\n",
        "        distance_array= jnp.array([clockwise_distance,anticlockwise_distance])\n",
        "        idx = jnp.argmin(jnp.abs(distance_array))\n",
        "        angular_distance = jnp.where(value==0.0,0.0,distance_array[idx])\n",
        "        #pred_value = jnp.where(jnp.sum(d_p) == 0.0, 0, items.direction)\n",
        "        sensor_obs.append(angular_distance)\n",
        "        distance_obs.append(value_dist)\n",
        "        #predator_obs.append(pred_value)\n",
        "        #prova_obs.append(ff)\n",
        "        #print(sensor_obs)\n",
        "        #print(prova_obs)\n",
        "        #input(\"whw\")\n",
        "    pred_angle = (pred_angle+jnp.pi) +jnp.pi\n",
        "    clockwise_distance_predator = (agent_dir+jnp.pi)-(pred_angle)\n",
        "    clockwise_distance_predator = jnp.where(clockwise_distance_predator > jnp.pi * 2, clockwise_distance_predator - (jnp.pi * 2),clockwise_distance_predator)\n",
        "    clockwise_distance_predator = jnp.where(clockwise_distance_predator < 0.0, clockwise_distance_predator + (jnp.pi * 2), clockwise_distance_predator)\n",
        "    anticlockwise_distance_predator = clockwise_distance_predator - (jnp.pi*2)\n",
        "    distance_array = jnp.array([clockwise_distance_predator, anticlockwise_distance_predator])\n",
        "    idx = jnp.argmin(jnp.abs(distance_array))\n",
        "    angular_distance = distance_array[idx]\n",
        "    predator_angle = jnp.stack(angular_distance*dist_pred_binary)\n",
        "    predator_distance = jnp.where(angular_distance*dist_pred_binary==0.0,0.0,dist_pred/(treshold_dist*2.0))\n",
        "    sensor_obs = jnp.stack(sensor_obs)\n",
        "    distance_obs = jnp.stack(distance_obs)\n",
        "    predator_distance = jnp.stack(predator_distance)\n",
        "   # predator_angle = jnp.stack(predator_angle)\n",
        "\n",
        "\n",
        "    return jnp.mean(distance_obs),jnp.concatenate([sensor_obs.ravel(),predator_angle.ravel()])#jnp.concatenate([sensor_obs.ravel(), pos_xy,eating,perc_angle,comm], axis=0)\n",
        "\n",
        "@jax.vmap\n",
        "def select_direction(key: jnp.ndarray, action_prob: jnp.ndarray) -> jnp.int32:\n",
        "    return random.choice(key, 4, replace=False, p=action_prob.ravel())\n",
        "\n",
        "\n",
        "class PredatorFlocking(VectorizedTask):\n",
        "    \"\"\"Water world, multi-agents training version.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_agents: int = 16,\n",
        "                 num_items: int = 1,\n",
        "                 max_steps: int = 1000,\n",
        "                 test: bool = False):\n",
        "\n",
        "        self.multi_agent_training = True\n",
        "        self.num_agents=num_agents\n",
        "\n",
        "        self.max_steps = max_steps\n",
        "        self.test = test\n",
        "        # num range sensor + position(x,y) + eating sensor + comm sensors\n",
        "        self.obs_shape = tuple([\n",
        "            num_agents, (NUM_RANGE_SENSORS -1) + 1 , ])\n",
        "        self.act_shape = tuple([num_agents, 1])\n",
        "        walls = jnp.array([[0, 0, 0, SCREEN_H],\n",
        "                           [0, SCREEN_H, SCREEN_W, SCREEN_H],\n",
        "                           [SCREEN_W, SCREEN_H, SCREEN_W, 0],\n",
        "                           [SCREEN_W, 0, 0, 0]])\n",
        "\n",
        "        def reset_fn(key):\n",
        "            next_key, key = random.split(key)\n",
        "            ks = random.split(key, num_agents + num_items)\n",
        "            agents = create_bubbles(ks[:num_agents], True)\n",
        "            items = create_bubbles(ks[num_agents:], False)\n",
        "\n",
        "            _,obs = get_obs(agents, agents, items, walls)\n",
        "            return State(agent_state=agents, item_state=items, obs=obs,\n",
        "                         steps=jnp.zeros((), dtype=jnp.int32), key=next_key)\n",
        "\n",
        "        self._reset_fn = jax.jit(jax.vmap(reset_fn))\n",
        "\n",
        "        def step_fn(state, action):\n",
        "\n",
        "            next_key, key = random.split(state.key)\n",
        "            ks = random.split(key, num_agents)\n",
        "            ki = random.split(key, num_items+1)\n",
        "            #action_keys = random.split(key, num_agents)\n",
        "            directions = state.obs#action#select_direction(action_keys, action)\n",
        "\n",
        "            agents = update_agent_state(state.agent_state,action,ks)\n",
        "\n",
        "            #items = update_item_state(state.item_state)\n",
        "            agt_x = agents.pos_x.ravel()\n",
        "            agt_y = agents.pos_y.ravel()\n",
        "\n",
        "            #items, rewards = get_item_move(agt_x,agt_y, state.item_state)\n",
        "\n",
        "\n",
        "            steps = state.steps + 1\n",
        "            done = jnp.where(steps >= max_steps, 1, 0)\n",
        "            distances, obs = get_obs(agents, agents, state.item_state, walls)\n",
        "            agents, items, rewards = get_rewards(state.item_state, agents, ki,distances)\n",
        "\n",
        "            # rewards=0\n",
        "            # items_state.shape=(num_agents, num_items), merge to (num_items, ).\n",
        "            agents = BubbleStatus(\n",
        "                pos_x=agents.pos_x[0], pos_y=agents.pos_y[0],\n",
        "                vel_x=agents.vel_x[0], vel_y=agents.vel_y[0],\n",
        "                bubble_type=agents.bubble_type[0],\n",
        "                direction=agents.direction[0],\n",
        "                valid=jnp.prod(agents.valid, axis=0))\n",
        "            return State(agent_state=agents, item_state=items, obs=obs,\n",
        "                         steps=steps, key=next_key), rewards, done\n",
        "\n",
        "        self._step_fn = jax.jit(jax.vmap(step_fn))\n",
        "\n",
        "    def reset(self, key: jnp.array) -> State:\n",
        "        return self._reset_fn(key)\n",
        "\n",
        "    def step(self,\n",
        "             state: State,\n",
        "             action: jnp.ndarray) -> Tuple[State, jnp.ndarray, jnp.ndarray]:\n",
        "        return self._step_fn(state, action)\n",
        "\n",
        "    @staticmethod\n",
        "    def render(state: State, task_id: int = 0) -> Image:\n",
        "        no_printvariables = 5\n",
        "        img = Image.new('RGB', (int(SCREEN_W*SCALE), int(SCREEN_H*SCALE)), (255, 255, 255))\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        state = tree_util.tree_map(lambda s: s[task_id], state)\n",
        "        # Draw the items.\n",
        "\n",
        "        items = state.item_state\n",
        "\n",
        "        for v, t, x, y,d in zip(np.array(items.valid, dtype=bool),\n",
        "                              np.array(items.bubble_type, dtype=int),\n",
        "                              np.array(items.pos_x),\n",
        "                              np.array(items.pos_y),\n",
        "                              np.array(items.direction)):\n",
        "            if v:\n",
        "                color = (255, 0, 0) if t == TYPE_FOOD else (255, 0, 0)\n",
        "                draw.ellipse(\n",
        "                    ((x - EATING_RADIUS*2)*SCALE, (y - EATING_RADIUS*2)*SCALE,\n",
        "                     (x + EATING_RADIUS*2)*SCALE, (y + EATING_RADIUS*2)*SCALE,),\n",
        "                    fill=color, outline=(0, 0, 0))\n",
        "\n",
        "\n",
        "\n",
        "        # Draw the agent.\n",
        "        agents = state.agent_state\n",
        "\n",
        "        for i, (v,x, y,d) in enumerate(zip(agents.valid,agents.pos_x, agents.pos_y,agents.direction)):\n",
        "            #for j, obs in enumerate(sensor_data[i]):\n",
        "            #    ang = j * DELTA_ANG - (np.pi/2)\n",
        "            #    dist = np.min(obs[:])\n",
        "            #    x_end = x + dist * MAX_RANGE * np.cos(ang)\n",
        "            #    y_end = y + dist * MAX_RANGE * np.sin(ang)\n",
        "            #    draw.line((x, y, x_end, y_end), fill=(0, 0, 0), width=1)\n",
        "           if v:\n",
        "                draw.ellipse(\n",
        "                    ((x - BUBBLE_RADIUS * 2)*SCALE, (y - BUBBLE_RADIUS * 2)*SCALE,\n",
        "                     (x + BUBBLE_RADIUS * 2)*SCALE, (y + BUBBLE_RADIUS * 2)*SCALE),\n",
        "                    fill=(0, 255, 0), outline=(0, 0, 0))\n",
        "\n",
        "\n",
        "                x_end = x + (2* BUBBLE_RADIUS)  * np.cos(d)\n",
        "                y_end = y - (2* BUBBLE_RADIUS) * np.sin(d)\n",
        "\n",
        "                draw.line((x*SCALE, y*SCALE, x_end*SCALE, y_end*SCALE), fill=(0, 0, 0), width=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZdVZaJ-zq7A",
        "outputId": "a861a00e-a100-4804-9d8d-1d2901e7e51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-d5f99c8a69dc>:334: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y4: jnp.float32) -> Tuple[np.bool, jnp.ndarray]:\n",
            "<ipython-input-4-d5f99c8a69dc>:362: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y3: jnp.float32) -> Tuple[np.bool, jnp.ndarray]:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2022 The EvoJAX Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "from functools import partial\n",
        "from typing import Tuple\n",
        "from typing import Union\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.tree_util import tree_map\n",
        "\n",
        "from evojax.obs_norm import ObsNormalizer\n",
        "from evojax.task.base import TaskState\n",
        "from evojax.task.base import VectorizedTask\n",
        "from evojax.policy.base import PolicyState\n",
        "from evojax.policy.base import PolicyNetwork\n",
        "from evojax.util import create_logger\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(1, 2, 3, 4, 5))\n",
        "def get_task_reset_keys(key: jnp.ndarray,\n",
        "                        test: bool,\n",
        "                        pop_size: int,\n",
        "                        n_tests: int,\n",
        "                        n_repeats: int,\n",
        "                        ma_training: bool) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    key, subkey = random.split(key=key)\n",
        "    if ma_training:\n",
        "        reset_keys = random.split(subkey, n_repeats)\n",
        "        reset_keys = jnp.tile(reset_keys, (pop_size, 1))\n",
        "    else:\n",
        "        if test:\n",
        "            reset_keys = random.split(subkey, n_tests * n_repeats)\n",
        "\n",
        "        else:\n",
        "            reset_keys = random.split(subkey, n_repeats)\n",
        "            reset_keys = jnp.tile(reset_keys, (pop_size, 1))\n",
        "    return key, reset_keys\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def split_params_for_pmap(param: jnp.ndarray) -> jnp.ndarray:\n",
        "    return jnp.stack(jnp.split(param, jax.local_device_count()))\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def split_states_for_pmap(\n",
        "        state: Union[TaskState, PolicyState]) -> Union[TaskState, PolicyState]:\n",
        "    return tree_map(split_params_for_pmap, state)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def reshape_data_from_pmap(data: jnp.ndarray) -> jnp.ndarray:\n",
        "    # data.shape = (#device, steps, #jobs/device, *)\n",
        "    data = data.transpose([1, 0] + [i for i in range(2, data.ndim)])\n",
        "    return jnp.reshape(data, (data.shape[0], data.shape[1] * data.shape[2], -1))\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(1, 2))\n",
        "def duplicate_params(params: jnp.ndarray,\n",
        "                     repeats: int,\n",
        "                     ma_training: bool) -> jnp.ndarray:\n",
        "    if ma_training:\n",
        "        return jnp.repeat(params, repeats= repeats,\n",
        "                          axis=0)  # jnp.tile(params, (repeats, ) + (1,) * (params.ndim - 1))\n",
        "    else:\n",
        "        return jnp.repeat(params, repeats=repeats, axis=0)\n",
        "\n",
        "\n",
        "class MSimManager(object):\n",
        "    \"\"\"Simulation manager.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_repeats: int,\n",
        "                 test_n_repeats: int,\n",
        "                 pop_size: int,\n",
        "                 agents: int,\n",
        "                 n_evaluations: int,\n",
        "                 policy_net: PolicyNetwork,\n",
        "                 train_vec_task: VectorizedTask,\n",
        "                 valid_vec_task: VectorizedTask,\n",
        "                 seed: int = 0,\n",
        "                 obs_normalizer: ObsNormalizer = None,\n",
        "                 logger: logging.Logger = None):\n",
        "        \"\"\"Initialization function.\n",
        "\n",
        "        Args:\n",
        "            n_repeats - Number of repeated parameter evaluations.\n",
        "            pop_size - Population size.\n",
        "            n_evaluations - Number of evaluations of the best parameter.\n",
        "            policy_net - Policy network.\n",
        "            train_vec_task - Vectorized tasks for training.\n",
        "            valid_vec_task - Vectorized tasks for validation.\n",
        "            seed - Random seed.\n",
        "            obs_normalizer - Observation normalization helper.\n",
        "            logger - Logger.\n",
        "        \"\"\"\n",
        "\n",
        "        if logger is None:\n",
        "            self._logger = create_logger(name='SimManager')\n",
        "        else:\n",
        "            self._logger = logger\n",
        "\n",
        "        self._key = random.PRNGKey(seed=seed)\n",
        "        self._n_repeats = n_repeats\n",
        "        self._test_n_repeats = test_n_repeats\n",
        "        self._pop_size = pop_size\n",
        "        self._agents = agents\n",
        "        self._n_evaluations = max(n_evaluations, jax.local_device_count())\n",
        "        self._ma_training = train_vec_task.multi_agent_training\n",
        "        self.rendering = []\n",
        "\n",
        "        self.obs_normalizer = obs_normalizer\n",
        "        if self.obs_normalizer is None:\n",
        "            self.obs_normalizer = ObsNormalizer(\n",
        "                obs_shape=train_vec_task.obs_shape,\n",
        "                dummy=True,\n",
        "            )\n",
        "        self.obs_params = self.obs_normalizer.get_init_params()\n",
        "        self.num_obs = train_vec_task.obs_shape[-1]+1\n",
        "        self._num_device = jax.local_device_count()\n",
        "        if self._pop_size % self._num_device != 0:\n",
        "            raise ValueError(\n",
        "                'pop_size must be multiples of GPU/TPUs: '\n",
        "                'pop_size={}, #devices={}'.format(\n",
        "                    self._pop_size, self._num_device))\n",
        "        if self._n_evaluations % self._num_device != 0:\n",
        "            raise ValueError(\n",
        "                'n_evaluations must be multiples of GPU/TPUs: '\n",
        "                'n_evaluations={}, #devices={}'.format(\n",
        "                    self._n_evaluations, self._num_device))\n",
        "\n",
        "        def step_once(carry, input_data, task):\n",
        "            (task_state, policy_state, params, obs_params,\n",
        "             accumulated_reward, valid_mask) = carry\n",
        "            if task.multi_agent_training:\n",
        "                num_tasks, num_agents = task_state.obs.shape[:2]\n",
        "                # print(num_tasks,num_agents)\n",
        "                # input(\"www\")\n",
        "                task_state = task_state.replace(\n",
        "                    obs=task_state.obs.reshape((-1, *task_state.obs.shape[2:])))\n",
        "\n",
        "            org_obs = task_state.obs\n",
        "            \n",
        "            #normed_obs = self.obs_normalizer.normalize_obs(org_obs, obs_params)\n",
        "            #task_state = task_state.replace(obs=normed_obs)\n",
        "            #actions, policy_state = policy_net.get_actions(\n",
        "            #    task_state, params, policy_state)\n",
        "            #actions, policy_state = policy_net.get_actions(\n",
        "            #    task_state, params[:,:-self.num_obs], policy_state)\n",
        "            action_f = jnp.sum(org_obs[:, :-1] * params[:, -self.num_obs:-1], axis=1)\n",
        "            action_p = org_obs[:, -1] * params[:, -1]\n",
        "            actionss = jnp.expand_dims((action_f + action_p),axis=1)\n",
        "\n",
        "            \n",
        "            \n",
        "            #actions = jnp.expand_dims((jnp.mean(org_obs[:,:-1],axis=1) + org_obs[:,-1])/2,axis=-1)\n",
        "            actions = actionss\n",
        "            \n",
        "            if task.multi_agent_training:\n",
        "                task_state = task_state.replace(\n",
        "                    obs=task_state.obs.reshape(\n",
        "                        (num_tasks, num_agents, *task_state.obs.shape[1:])))\n",
        "                actions = actions.reshape(\n",
        "                    (num_tasks, num_agents, *actions.shape[1:]))\n",
        "\n",
        "            task_state, reward, done = task.step(task_state, actions)\n",
        "            \n",
        "            if task.multi_agent_training:\n",
        "                reward = reward.ravel()\n",
        "                reward = jnp.repeat(reward, num_agents, axis=0)\n",
        "                done = jnp.repeat(done, num_agents, axis=0)\n",
        "\n",
        "            accumulated_reward = accumulated_reward + reward * valid_mask\n",
        "            valid_mask = valid_mask * (1 - done.ravel())\n",
        "\n",
        "            return ((task_state, policy_state, params, obs_params,\n",
        "                     accumulated_reward, valid_mask),\n",
        "                    (org_obs, valid_mask))\n",
        "\n",
        "        def rollout(task_states, policy_states, params, obs_params,\n",
        "                    step_once_fn, max_steps):\n",
        "            accumulated_rewards = jnp.zeros(params.shape[0])\n",
        "            valid_masks = jnp.ones(params.shape[0])\n",
        "\n",
        "            ((task_states, policy_states, params, obs_params,\n",
        "              accumulated_rewards, valid_masks),\n",
        "             (obs_set, obs_mask)) = jax.lax.scan(\n",
        "                step_once_fn,\n",
        "                (task_states, policy_states, params, obs_params,\n",
        "                 accumulated_rewards, valid_masks), (), max_steps)\n",
        "\n",
        "            return accumulated_rewards, obs_set, obs_mask\n",
        "\n",
        "        def rollout_test(task_states, policy_states, params, obs_params,\n",
        "                    step_once_fn, max_steps):\n",
        "            accumulated_rewards = jnp.zeros(params.shape[0])\n",
        "            self.rendering.append(train_vec_task.render(task_states))\n",
        "\n",
        "            valid_masks = jnp.ones(params.shape[0])\n",
        "\n",
        "            ((task_states, policy_states, params, obs_params,\n",
        "              accumulated_rewards, valid_masks),\n",
        "             (obs_set, obs_mask)) = jax.lax.scan(\n",
        "                step_once_fn,\n",
        "                (task_states, policy_states, params, obs_params,\n",
        "                 accumulated_rewards, valid_masks), (), max_steps)\n",
        "\n",
        "            return accumulated_rewards, obs_set, obs_mask\n",
        "\n",
        "        self.policy = policy_net\n",
        "        self.test_task = valid_vec_task\n",
        "        self._policy_reset_fn = jax.jit(policy_net.reset)\n",
        "\n",
        "        # Set up training functions.\n",
        "        self._train_reset_fn = train_vec_task.reset\n",
        "\n",
        "        self._train_rollout_fn = partial(\n",
        "            rollout,\n",
        "            step_once_fn=partial(step_once, task=train_vec_task),\n",
        "            max_steps=train_vec_task.max_steps)\n",
        "        if self._num_device > 1:\n",
        "            self._train_rollout_fn = jax.jit(jax.pmap(\n",
        "                self._train_rollout_fn, in_axes=(0, 0, 0, None)))\n",
        "            \n",
        "\n",
        "        # Set up validation functions.\n",
        "        self._valid_reset_fn = valid_vec_task.reset\n",
        "        self._valid_rollout_fn = partial(\n",
        "            rollout,\n",
        "            step_once_fn=partial(step_once, task=valid_vec_task),\n",
        "            max_steps=valid_vec_task.max_steps)\n",
        "        if self._num_device > 1:\n",
        "            self._valid_rollout_fn = jax.jit(jax.pmap(\n",
        "                self._valid_rollout_fn, in_axes=(0, 0, 0, None)))\n",
        "\n",
        "    def eval_params(self, params: jnp.ndarray, test: bool) -> jnp.ndarray:\n",
        "        \"\"\"Evaluate population parameters or test the best parameter.\n",
        "\n",
        "        Args:\n",
        "            params - Parameters to be evaluated.\n",
        "            test - Whether we are testing the best parameter\n",
        "        Returns:\n",
        "            An array of fitness scores.\n",
        "        \"\"\"\n",
        "\n",
        "        policy_reset_func = self._policy_reset_fn\n",
        "        if test:\n",
        "            n_repeats = self._test_n_repeats\n",
        "            task_reset_func = self._valid_reset_fn\n",
        "            rollout_func = self._valid_rollout_fn\n",
        "            params = duplicate_params(\n",
        "                params[None, :], self._pop_size, False)\n",
        "\n",
        "        else:\n",
        "            n_repeats = self._n_repeats\n",
        "            task_reset_func = self._train_reset_fn\n",
        "\n",
        "            rollout_func = self._train_rollout_fn\n",
        "\n",
        "        # Suppose pop_size=2 and n_repeats=3.\n",
        "        # For multi-agents training, params become\n",
        "        #   a1, a2, ..., an  (individual 1 params)\n",
        "        #   b1, b2, ..., bn  (individual 2 params)\n",
        "        #   a1, a2, ..., an  (individual 1 params)\n",
        "        #   b1, b2, ..., bn  (individual 2 params)\n",
        "        #   a1, a2, ..., an  (individual 1 params)\n",
        "        #   b1, b2, ..., bn  (individual 2 params)\n",
        "        # For non-ma training, params become\n",
        "        #   a1, a2, ..., an  (individual 1 params)\n",
        "        #   a1, a2, ..., an  (individual 1 params)\n",
        "        #   a1, a2, ..., an  (individual 1 params)\n",
        "        #   b1, b2, ..., bn  (individual 2 params)\n",
        "        #   b1, b2, ..., bn  (individual 2 params)\n",
        "        #   b1, b2, ..., bn  (individual 2 params)\n",
        "        #print(params)\n",
        "        #print(\"prima\")\n",
        "        params = duplicate_params(params, self._agents*n_repeats, self._ma_training)\n",
        "\n",
        "        #print(params)\n",
        "        #print(params[:10,:])\n",
        "        #input(\"ee\")\n",
        "        \n",
        "        self._key, reset_keys = get_task_reset_keys(\n",
        "            self._key, test, self._pop_size, self._n_evaluations, n_repeats,\n",
        "            self._ma_training)\n",
        "\n",
        "        if test:\n",
        "            params = params[:self._agents * n_repeats, :]\n",
        "            \n",
        "            reset_keys = reset_keys[:n_repeats, :]\n",
        "\n",
        "        #print(self._key,reset_keys)\n",
        "        #input(\"key\")\n",
        "        # Reset the tasks and the policy.\n",
        "        #print(jnp.shape(reset_keys),jnp.shape(params))\n",
        "        \n",
        "        task_state = task_reset_func(reset_keys)\n",
        "\n",
        "        policy_state = policy_reset_func(task_state)\n",
        "\n",
        "        if self._num_device > 1:\n",
        "            params = split_params_for_pmap(params)\n",
        "            task_state = split_states_for_pmap(task_state)\n",
        "            policy_state = split_states_for_pmap(policy_state)\n",
        "\n",
        "        # Do the rollouts.\n",
        "\n",
        "        scores, all_obs, masks = rollout_func(\n",
        "              task_state, policy_state, params, self.obs_params)\n",
        "        \n",
        "\n",
        "        if self._num_device > 1:\n",
        "            all_obs = reshape_data_from_pmap(all_obs)\n",
        "            masks = reshape_data_from_pmap(masks)\n",
        "\n",
        "        if not test and not self.obs_normalizer.is_dummy:\n",
        "            \n",
        "            self.obs_params = self.obs_normalizer.update_normalization_params(\n",
        "                obs_buffer=all_obs, obs_mask=masks, obs_params=self.obs_params)\n",
        "\n",
        "        if self._ma_training:\n",
        "            if not test:\n",
        "                # In training, each agent has different parameters.\n",
        "                # return jnp.mean(scores.ravel().reshape((n_repeats, -1)), axis=0)\n",
        "                tmp = scores.ravel().reshape((self._pop_size * n_repeats, -1))\n",
        "                \n",
        "                tmp = jnp.mean(tmp, axis=1).reshape(self._pop_size, n_repeats)\n",
        "                  \n",
        "                return jnp.mean(tmp, axis=1)\n",
        "            else:\n",
        "\n",
        "                tmp = scores.ravel().reshape((n_repeats, self._agents))\n",
        "                print(jnp.mean(tmp, axis=1))\n",
        "                gif_file = os.path.join('/content/gdrive/My Drive/log/water_world_ma', 'base_scores.npy')\n",
        "                np.save(gif_file,jnp.mean(tmp, axis=1))\n",
        "                \n",
        "                # In tests, they share the same parameters.\n",
        "                return jnp.mean(tmp, axis=1)\n",
        "        else:\n",
        "            return jnp.mean(scores.ravel().reshape((-1, n_repeats)), axis=-1)"
      ],
      "metadata": {
        "id": "k1kIcm3vPrwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2022 The EvoJAX Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from evojax.task.base import VectorizedTask\n",
        "from evojax.policy import PolicyNetwork\n",
        "from evojax.algo import NEAlgorithm\n",
        "from evojax.sim_mgr import SimManager\n",
        "from evojax.obs_norm import ObsNormalizer\n",
        "from evojax.util import create_logger\n",
        "from evojax.util import load_model\n",
        "from evojax.util import save_model\n",
        "\n",
        "\n",
        "class MTrainer(object):\n",
        "    \"\"\"A trainer that organizes the training logistics.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 policy: PolicyNetwork,\n",
        "                 solver: NEAlgorithm,\n",
        "                 train_task: VectorizedTask,\n",
        "                 test_task: VectorizedTask,\n",
        "                 max_iter: int = 1000,\n",
        "                 log_interval: int = 20,\n",
        "                 test_interval: int = 100,\n",
        "                 n_repeats: int = 1,\n",
        "                 test_n_repeats: int = 1,\n",
        "                 n_evaluations: int = 100,\n",
        "                 num_agents: int = 64,\n",
        "                 seed: int = 42,\n",
        "                 debug: bool = False,\n",
        "                 normalize_obs: bool = False,\n",
        "                 model_dir: str = None,\n",
        "                 log_dir: str = None,\n",
        "                 logger: logging.Logger = None):\n",
        "        \"\"\"Initialization.\n",
        "\n",
        "        Args:\n",
        "            policy - The policy network to use.\n",
        "            solver - The ES algorithm for optimization.\n",
        "            train_task - The task for training.\n",
        "            test_task - The task for evaluation.\n",
        "            max_iter - Maximum number of training iterations.\n",
        "            log_interval - Interval for logging.\n",
        "            test_interval - Interval for tests.\n",
        "            n_repeats - Number of rollout repetitions.\n",
        "            n_evaluations - Number of tests to conduct.\n",
        "            seed - Random seed to use.\n",
        "            debug - Whether to turn on the debug flag.\n",
        "            normalize_obs - Whether to use an observation normalizer.\n",
        "            model_dir - Directory to save/load model.\n",
        "            log_dir - Directory to dump logs.\n",
        "            logger - Logger.\n",
        "        \"\"\"\n",
        "\n",
        "        if logger is None:\n",
        "            self._logger = create_logger(\n",
        "                name='Trainer', log_dir=log_dir, debug=debug)\n",
        "        else:\n",
        "            self._logger = logger\n",
        "\n",
        "        self._log_interval = log_interval\n",
        "        self._test_interval = test_interval\n",
        "        self._max_iter = max_iter\n",
        "        self.model_dir = model_dir\n",
        "        self._log_dir = log_dir\n",
        "        self.seed = seed\n",
        "        self._obs_normalizer = ObsNormalizer(\n",
        "            obs_shape=train_task.obs_shape,\n",
        "            dummy=not normalize_obs,\n",
        "        )\n",
        "\n",
        "        self.solver = solver\n",
        "        self.task = test_task\n",
        "        self.policy = policy\n",
        "        self.sim_mgr = MSimManager(\n",
        "            n_repeats=n_repeats,\n",
        "            test_n_repeats=test_n_repeats,\n",
        "            pop_size=solver.pop_size,\n",
        "            agents=num_agents,\n",
        "            n_evaluations=n_evaluations,\n",
        "            policy_net=policy,\n",
        "            train_vec_task=train_task,\n",
        "            valid_vec_task=test_task,\n",
        "            seed=seed,\n",
        "            obs_normalizer=self._obs_normalizer,\n",
        "            logger=self._logger,\n",
        "        )\n",
        "\n",
        "    def run(self, demo_mode: bool = False) -> float:\n",
        "        \"\"\"Start the training / test process.\"\"\"\n",
        "\n",
        "        if self.model_dir is not None:\n",
        "            params, obs_params = load_model(model_dir=self.model_dir)\n",
        "\n",
        "            self.sim_mgr.obs_params = obs_params\n",
        "            self._logger.info(\n",
        "                'Loaded model parameters from {}.'.format(self.model_dir))\n",
        "        else:\n",
        "            params = None\n",
        "\n",
        "        if demo_mode:\n",
        "            if params is None:\n",
        "                raise ValueError('No policy parameters to evaluate.')\n",
        "            self._logger.info('Start to test the parameters.')\n",
        "            scores = np.array(\n",
        "                self.sim_mgr.eval_params(params=params, test=True))\n",
        "            self._logger.info(\n",
        "                '[TEST] #tests={0}, max={1:.4f}, avg={2:.4f}, min={3:.4f}, '\n",
        "                'std={4:.4f}'.format(scores.size, scores.max(), scores.mean(),\n",
        "                                     scores.min(), scores.std()))\n",
        "            return scores.mean()\n",
        "        else:\n",
        "            self._logger.info(\n",
        "                'Start to train for {} iterations.'.format(self._max_iter))\n",
        "\n",
        "            if params is not None:\n",
        "                # Continue training from the breakpoint.\n",
        "                self.solver.best_params = params\n",
        "                best_params = params\n",
        "\n",
        "            best_score = -float('Inf')\n",
        "            fitness_ = []\n",
        "            for i in range(self._max_iter):\n",
        "\n",
        "                start_time = time.perf_counter()\n",
        "                params = self.solver.ask()\n",
        "                \n",
        "                self._logger.debug('solver.ask time: {0:.4f}s'.format(\n",
        "                    time.perf_counter() - start_time))\n",
        "\n",
        "                start_time = time.perf_counter()\n",
        "                scores = self.sim_mgr.eval_params(params=params, test=False)\n",
        "\n",
        "                self._logger.debug('sim_mgr.eval_params time: {0:.4f}s'.format(\n",
        "                    time.perf_counter() - start_time))\n",
        "\n",
        "                start_time = time.perf_counter()\n",
        "                self.solver.tell(fitness=scores)\n",
        "                self._logger.debug('solver.tell time: {0:.4f}s'.format(\n",
        "                    time.perf_counter() - start_time))\n",
        "\n",
        "                if i > 0 and i % self._log_interval == 0:\n",
        "                    scores = np.array(scores)\n",
        "\n",
        "                    best_ind = np.argmax(scores)\n",
        "\n",
        "                    if scores[best_ind] > best_score:\n",
        "                        best_params = params[best_ind]\n",
        "                        best_score = scores[best_ind]\n",
        "                        print(\"best_params\",best_params)\n",
        "\n",
        "                    print(\"iter \",i,\"size\",scores.size,\"max\",scores.max())\n",
        "                    fitness_.append(scores.max())\n",
        "                    self._logger.info(\n",
        "                        'Iter={0}, size={1}, max={2:.4f}, '\n",
        "                        'avg={3:.4f}, min={4:.4f}, std={5:.4f}'.format(\n",
        "                            i, scores.size, scores.max(), scores.mean(),\n",
        "                            scores.min(), scores.std()))\n",
        "\n",
        "                if i > 0 and i % self._test_interval == 0:\n",
        "                    test_params = params[best_ind]  # self.solver.best_params\n",
        "                    \n",
        "                    test_scores = self.sim_mgr.eval_params(\n",
        "                        params=test_params, test=True)\n",
        "                    print(\n",
        "                        '[TEST] Iter={0}, #tests={1}, max={2:.4f} avg={3:.4f}, '\n",
        "                        'min={4:.4f}, std={5:.4f}'.format(\n",
        "                            i, test_scores.size, test_scores.max(),\n",
        "                            test_scores.mean(), test_scores.min(),\n",
        "                            test_scores.std()))\n",
        "                    mean_test_score = test_scores.mean()\n",
        "                    # save_model(\n",
        "                    #    model_dir=self._log_dir,\n",
        "                    #    model_name='iter_{}'.format(i),\n",
        "                    #    params=best_params,\n",
        "                    #    obs_params=self.sim_mgr.obs_params,\n",
        "                    #    best=mean_test_score > best_score,\n",
        "                    # )\n",
        "                    if mean_test_score > best_score:\n",
        "                        best_params = test_params\n",
        "                        best_score = mean_test_score\n",
        "                    best_score = max(best_score, mean_test_score)\n",
        "\n",
        "            # Test and save the final model.\n",
        "            self.solver.best_params = best_params\n",
        "            self.best_params = self.solver.best_params  # best_params#self.solver.best_params\n",
        "\n",
        "            test_scores = self.sim_mgr.eval_params(\n",
        "                params=self.best_params, test=True)\n",
        "            print(\n",
        "                '[TEST] Iter={0}, #tests={1}, max={2:.4f}, avg={3:.4f}, '\n",
        "                'min={4:.4f}, std={5:.4f}'.format(\n",
        "                    self._max_iter, test_scores.size, test_scores.max(),\n",
        "                    test_scores.mean(), test_scores.min(), test_scores.std()))\n",
        "            mean_test_score = test_scores.mean()\n",
        "            save_model(\n",
        "                model_dir=self._log_dir,\n",
        "                model_name='final'+str(self.seed),\n",
        "                params=self.best_params,\n",
        "                obs_params=self.sim_mgr.obs_params,\n",
        "                best=mean_test_score > best_score,\n",
        "            )\n",
        "            np.save(self._log_dir+\"/fitness\"+str(self.seed)+\".npy\",fitness_)\n",
        "            best_score = max(best_score, mean_test_score)\n",
        "            self._logger.info(\n",
        "                'Training done, best_score={0:.4f}'.format(best_score))\n",
        "\n",
        "            return best_score\n"
      ],
      "metadata": {
        "id": "6Fkgv6VeMMF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xBeHOBTkjzQ",
        "outputId": "c8c8aa74-006d-4ae3-b345-543a4b595328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Let's create a directory to save logs and models.\n",
        "log_dir = './log'\n",
        "logger = create_logger(name='EvoJAX', log_dir=log_dir)\n",
        "logger.info('Welcome to the tutorial on Neuroevolution algorithm creation!')\n",
        "\n",
        "logger.info('Jax backend: {}'.format(jax.local_devices()))\n",
        "\n",
        "!nvidia-smi --query-gpu=name --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OUo9J48GUm0"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlwedOB_bQn-"
      },
      "source": [
        "EvoJAX has three major components: the *task*, the *policy network* and the *neuroevolution algorithm*. Once these components are implemented and instantiated, we can use a trainer to start the training process. The following code snippet provides an example of how we use EvoJAX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8NuOjmKcj-F"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "#from evojax.task.ma_waterworld import MultiAgentWaterWorld\n",
        "from evojax.policy.mlp import MLPPolicy\n",
        "from evojax.algo import PGPE\n",
        "from evojax import Trainer\n",
        "from evojax import util\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--hidden-size', type=int, default=16, help='Policy hidden size.')\n",
        "    parser.add_argument(\n",
        "        '--num-tests', type=int, default=50, help='Number of test rollouts.')\n",
        "    parser.add_argument(\n",
        "        '--n-repeats', type=int, default=10, help='Training repetitions.')\n",
        "    parser.add_argument(\n",
        "        '--max-iter', type=int, default=50, help='Max training iterations.')\n",
        "    parser.add_argument(\n",
        "        '--test-interval', type=int, default=20, help='Test interval.')\n",
        "    parser.add_argument(\n",
        "        '--log-interval', type=int, default=1, help='Logging interval.')\n",
        "    parser.add_argument(\n",
        "        '--seed', type=int, default=2, help='Random seed for training.')\n",
        "    parser.add_argument(\n",
        "        '--center-lr', type=float, default=0.011, help='Center learning rate.')\n",
        "    parser.add_argument(\n",
        "        '--std-lr', type=float, default=0.054, help='Std learning rate.')\n",
        "    parser.add_argument(\n",
        "        '--init-std', type=float, default=0.095, help='Initial std.')#0.095\n",
        "    parser.add_argument(\n",
        "        '--gpu-id', type=str, help='GPU(s) to use.')\n",
        "    parser.add_argument(\n",
        "        '--debug', action='store_true', help='Debug mode.')\n",
        "    config, _ = parser.parse_known_args()\n",
        "    return config\n",
        "\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    log_dir = '/content/gdrive/My Drive/log/reactive/evolved'#'./log/water_world_ma'\n",
        "    \n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "    logger = util.create_logger(\n",
        "        name='MultiAgentWaterWorld', log_dir=log_dir, debug=config.debug)\n",
        "\n",
        "    logger.info('EvoJAX MultiAgentWaterWorld')\n",
        "    logger.info('=' * 30)\n",
        "    population = 40\n",
        "    num_agents = 512\n",
        "    max_steps = 400\n",
        "    #key = jax.random.PRNGKey(125)[None, :]\n",
        "    for ii in range(1,10):\n",
        "      config.seed = ii+1\n",
        "      train_task = PredatorFlocking(\n",
        "          num_agents=num_agents, test=False, max_steps=max_steps)\n",
        "      test_task = PredatorFlocking(\n",
        "          num_agents=num_agents, test=True, max_steps=max_steps)\n",
        "      #task_reset_fn = jax.jit(test_task.reset)\n",
        "      #task_state = task_reset_fn(key)\n",
        "      num_obs = 8#np.shape(task_state.obs)[-1]+1\n",
        "      policy = MLPPolicy(\n",
        "          input_dim=train_task.obs_shape[-1],\n",
        "          hidden_dims=[config.hidden_size,],\n",
        "          output_dim=train_task.act_shape[-1],\n",
        "          output_act_fn='tanh',\n",
        "      )\n",
        "      init_vector = np.hstack((np.ones(num_obs-1)*((1/(num_obs-1))*0.5),np.ones(1)*0.5))\n",
        "      solver = PGPE(\n",
        "          pop_size=population,\n",
        "          param_size= num_obs,\n",
        "          init_params=init_vector,\n",
        "          optimizer='adam',\n",
        "          center_learning_rate=config.center_lr,\n",
        "          stdev_learning_rate=config.std_lr,\n",
        "          init_stdev=config.init_std,\n",
        "          logger=logger,\n",
        "          seed=config.seed,\n",
        "      )\n",
        "\n",
        "\n",
        "      trainer = MTrainer(\n",
        "          policy=policy,\n",
        "          solver=solver,\n",
        "          train_task=train_task,\n",
        "          test_task=test_task,\n",
        "          max_iter=config.max_iter,\n",
        "          log_interval=config.log_interval,\n",
        "          test_interval=config.test_interval,\n",
        "          n_evaluations=config.num_tests,\n",
        "          num_agents = num_agents,\n",
        "          n_repeats=config.n_repeats,\n",
        "          test_n_repeats=config.num_tests,\n",
        "          seed=config.seed,\n",
        "          log_dir=log_dir,\n",
        "          logger=logger,\n",
        "      )\n",
        "      \n",
        "      trainer.run()\n",
        "    # Visualize the policy.\n",
        "    for j in range(10):\n",
        "      task_reset_fn = jax.jit(test_task.reset)\n",
        "      policy_reset_fn = jax.jit(policy.reset)\n",
        "      step_fn = jax.jit(test_task.step)\n",
        "      action_fn = jax.jit(policy.get_actions)\n",
        " \n",
        "      #best_params = jnp.repeat(\n",
        "      #    trainer.best_params, num_agents, axis=0)\n",
        "      \n",
        "      #best_params = best_params.reshape(num_agents,policy.num_params)\n",
        "      best_params = jnp.tile(\n",
        "        trainer.best_params, num_agents).reshape(num_agents,num_obs)\n",
        "      #print(trainer.best_params)\n",
        "      #input(\"pause\")\n",
        "\n",
        "      key = jax.random.PRNGKey(j)[None, :]\n",
        "\n",
        "      task_state = task_reset_fn(key)\n",
        "      policy_state = policy_reset_fn(task_state)\n",
        "      screens = []\n",
        "      rew = 0\n",
        "      for _ in range(200):\n",
        "          num_tasks, num_agents = task_state.obs.shape[:2]\n",
        "          task_state = task_state.replace(\n",
        "              obs=task_state.obs.reshape((-1, *task_state.obs.shape[2:])))\n",
        "          \n",
        "          #action, policy_state = action_fn(task_state, best_params, policy_state)\n",
        "          #action = action.reshape(num_tasks, num_agents, *action.shape[1:])\n",
        "          task_state = task_state.replace(\n",
        "              obs=task_state.obs.reshape(\n",
        "                  num_tasks, num_agents, *task_state.obs.shape[1:]))\n",
        "          org_obs = task_state.obs\n",
        "          #actions, policy_state = self.policy.get_actions(\n",
        "          #  task_state, params[:,:-self.num_obs], policy_state)\n",
        "          \n",
        "          #input(\"ww\")\n",
        "          action_f = jnp.sum(org_obs[0,:, :-1] * best_params[:,:-1], axis=1)\n",
        "          action_p = org_obs[0,:, -1] * best_params[:, -1]\n",
        "          actions = jnp.expand_dims((action_f* best_params[:, -1]  + action_p),axis=1)\n",
        "          actions = actions.reshape(num_tasks, num_agents, num_tasks)\n",
        "          \n",
        "          task_state, reward, done = step_fn(task_state, actions)\n",
        "          print(_,rew)\n",
        "          rew+=reward\n",
        "          screens.append(test_task.render(task_state))\n",
        "      print(rew)\n",
        "      #imgs = render(test_task, solver, policy)\n",
        "\n",
        "      gif_file = os.path.join(log_dir, 'DangerZone_100_m' +str(j)+'.gif')\n",
        "      screens[j].save(\n",
        "          gif_file, save_all=True, append_images=screens[1:], duration=40, loop=0)\n",
        "\n",
        "    #from google.colab import files\n",
        "    #files.download( './log/water_world_ma/cartpole.gif')\n",
        "    #Image(open(os.path.join(log_dir, 'cartpole.gif'),'rb').read())\n",
        "\n",
        "    \n",
        "'''\n",
        "if task.multi_agent_training:\n",
        "                num_tasks, num_agents = task_state.obs.shape[:2]\n",
        "                # print(num_tasks,num_agents)\n",
        "                # input(\"www\")\n",
        "                task_state = task_state.replace(\n",
        "                    obs=task_state.obs.reshape((-1, *task_state.obs.shape[2:])))\n",
        "\n",
        "            org_obs = task_state.obs\n",
        "\n",
        "            #normed_obs = self.obs_normalizer.normalize_obs(org_obs, obs_params)\n",
        "            #task_state = task_state.replace(obs=normed_obs)\n",
        "\n",
        "            actions, policy_state = policy_net.get_actions(\n",
        "                task_state, params, policy_state)\n",
        "\n",
        "\n",
        "            if task.multi_agent_training:\n",
        "                task_state = task_state.replace(\n",
        "                    obs=task_state.obs.reshape(\n",
        "                        (num_tasks, num_agents, *task_state.obs.shape[1:])))\n",
        "                actions = actions.reshape(\n",
        "                    (num_tasks, num_agents, *actions.shape[1:]))\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    configs = parse_args()\n",
        "    if configs.gpu_id is not None:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id\n",
        "    main(configs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "#from evojax.task.ma_waterworld import MultiAgentWaterWorld\n",
        "from evojax.policy.mlp import MLPPolicy\n",
        "from evojax.algo import PGPE\n",
        "from evojax import Trainer\n",
        "from evojax import util\n",
        "from evojax.util import load_model\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--hidden-size', type=int, default=16, help='Policy hidden size.')\n",
        "    parser.add_argument(\n",
        "        '--num-tests', type=int, default=50, help='Number of test rollouts.')\n",
        "    parser.add_argument(\n",
        "        '--n-repeats', type=int, default=2, help='Training repetitions.')\n",
        "    parser.add_argument(\n",
        "        '--max-iter', type=int, default=2, help='Max training iterations.')\n",
        "    parser.add_argument(\n",
        "        '--test-interval', type=int, default=1, help='Test interval.')\n",
        "    parser.add_argument(\n",
        "        '--log-interval', type=int, default=1, help='Logging interval.')\n",
        "    parser.add_argument(\n",
        "        '--seed', type=int, default=421, help='Random seed for training.')\n",
        "    parser.add_argument(\n",
        "        '--center-lr', type=float, default=0.011, help='Center learning rate.')\n",
        "    parser.add_argument(\n",
        "        '--std-lr', type=float, default=0.054, help='Std learning rate.')\n",
        "    parser.add_argument(\n",
        "        '--init-std', type=float, default=0.00000000000001, help='Initial std.')#0.095\n",
        "    parser.add_argument(\n",
        "        '--gpu-id', type=str, help='GPU(s) to use.')\n",
        "    parser.add_argument(\n",
        "        '--debug', action='store_true', help='Debug mode.')\n",
        "    config, _ = parser.parse_known_args()\n",
        "    return config\n",
        "\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    log_dir = '/content/gdrive/My Drive/log/reactive/evolved'#'./log/water_world_ma'\n",
        "    print(log_dir)\n",
        "    init_vector, obs_params = load_model(model_dir=log_dir+'/')\n",
        "    print(init_vector)\n",
        "    \n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "    logger = util.create_logger(\n",
        "        name='MultiAgentWaterWorld', log_dir=log_dir, debug=config.debug)\n",
        "\n",
        "    logger.info('EvoJAX MultiAgentWaterWorld')\n",
        "    logger.info('=' * 30)\n",
        "    population = 40\n",
        "    num_agents = 1024\n",
        "    max_steps = 400\n",
        "    #key = jax.random.PRNGKey(125)[None, :]\n",
        "    train_task = PredatorFlocking(\n",
        "        num_agents=num_agents, test=False, max_steps=max_steps)\n",
        "    test_task = PredatorFlocking(\n",
        "        num_agents=num_agents, test=True, max_steps=max_steps)\n",
        "    #task_reset_fn = jax.jit(test_task.reset)\n",
        "    #task_state = task_reset_fn(key)\n",
        "    num_obs = 8##np.shape(task_state.obs)[-1]+1\n",
        "    policy = MLPPolicy(\n",
        "        input_dim=train_task.obs_shape[-1],\n",
        "        hidden_dims=[config.hidden_size,],\n",
        "        output_dim=train_task.act_shape[-1],\n",
        "        output_act_fn='tanh',\n",
        "    )\n",
        "    #init_vector = np.ones(num_obs)*(1/(num_obs))\n",
        "    solver = PGPE(\n",
        "        pop_size=population,\n",
        "        param_size= num_obs,\n",
        "        init_params=init_vector,\n",
        "        optimizer='adam',\n",
        "        center_learning_rate=config.center_lr,\n",
        "        stdev_learning_rate=config.std_lr,\n",
        "        init_stdev=config.init_std,\n",
        "        logger=logger,\n",
        "        seed=config.seed,\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer = MTrainer(\n",
        "        policy=policy,\n",
        "        solver=solver,\n",
        "        train_task=train_task,\n",
        "        test_task=test_task,\n",
        "        max_iter=config.max_iter,\n",
        "        log_interval=config.log_interval,\n",
        "        test_interval=config.test_interval,\n",
        "        n_evaluations=config.num_tests,\n",
        "        num_agents = num_agents,\n",
        "        n_repeats=config.n_repeats,\n",
        "        test_n_repeats=config.num_tests,\n",
        "        seed=config.seed,\n",
        "        log_dir=log_dir,\n",
        "        model_dir=log_dir,\n",
        "        logger=logger,\n",
        "    )\n",
        "    best_params = init_vector\n",
        "    #trainer.run()\n",
        "        # Visualize the policy.\n",
        "    for j in range(10):\n",
        "      task_reset_fn = jax.jit(test_task.reset)\n",
        "      policy_reset_fn = jax.jit(policy.reset)\n",
        "      step_fn = jax.jit(test_task.step)\n",
        "      action_fn = jax.jit(policy.get_actions)\n",
        " \n",
        "      #best_params = jnp.repeat(\n",
        "      #    trainer.best_params, num_agents, axis=0)\n",
        "      \n",
        "      #best_params = best_params.reshape(num_agents,policy.num_params)\n",
        "      #best_params = jnp.tile(\n",
        "      #  trainer.best_params, num_agents).reshape(num_agents,num_obs)\n",
        "      #print(trainer.best_params)\n",
        "      #input(\"pause\")\n",
        "      best_params = jnp.tile(\n",
        "        best_params, num_agents).reshape(num_agents,num_obs)\n",
        "      key = jax.random.PRNGKey(j)[None, :]\n",
        "\n",
        "      task_state = task_reset_fn(key)\n",
        "      policy_state = policy_reset_fn(task_state)\n",
        "      screens = []\n",
        "      rew = 0\n",
        "      for _ in range(150):\n",
        "          num_tasks, num_agents = task_state.obs.shape[:2]\n",
        "          task_state = task_state.replace(\n",
        "              obs=task_state.obs.reshape((-1, *task_state.obs.shape[2:])))\n",
        "          \n",
        "          #action, policy_state = action_fn(task_state, best_params, policy_state)\n",
        "          #action = action.reshape(num_tasks, num_agents, *action.shape[1:])\n",
        "          task_state = task_state.replace(\n",
        "              obs=task_state.obs.reshape(\n",
        "                  num_tasks, num_agents, *task_state.obs.shape[1:]))\n",
        "          org_obs = task_state.obs\n",
        "          #actions, policy_state = self.policy.get_actions(\n",
        "          #  task_state, params[:,:-self.num_obs], policy_state)\n",
        "          \n",
        "          #input(\"ww\")\n",
        "          action_f = jnp.sum(org_obs[0,:, :-1] * best_params[:, -num_obs:-1], axis=1)\n",
        "          action_p = org_obs[0,:, -1] * best_params[:, -1]\n",
        "          actions = jnp.expand_dims((action_f + action_p),axis=1)\n",
        "          actions = actions.reshape(num_tasks, num_agents, num_tasks)\n",
        "          \n",
        "          task_state, reward, done = step_fn(task_state, actions)\n",
        "          print(_,rew)\n",
        "          rew+=reward\n",
        "          screens.append(test_task.render(task_state))\n",
        "      print(rew)\n",
        "      #imgs = render(test_task, solver, policy)\n",
        "\n",
        "      gif_file = os.path.join(log_dir, 'DangerZone_100_m' +str(j)+'.gif')\n",
        "      screens[j].save(\n",
        "          gif_file, save_all=True, append_images=screens[1:], duration=40, loop=0)\n",
        "\n",
        "    #from google.colab import files\n",
        "    #files.download( './log/water_world_ma/cartpole.gif')\n",
        "    #Image(open(os.path.join(log_dir, 'cartpole.gif'),'rb').read())\n",
        "\n",
        "    \n",
        "'''\n",
        "if task.multi_agent_training:\n",
        "                num_tasks, num_agents = task_state.obs.shape[:2]\n",
        "                # print(num_tasks,num_agents)\n",
        "                # input(\"www\")\n",
        "                task_state = task_state.replace(\n",
        "                    obs=task_state.obs.reshape((-1, *task_state.obs.shape[2:])))\n",
        "\n",
        "            org_obs = task_state.obs\n",
        "\n",
        "            #normed_obs = self.obs_normalizer.normalize_obs(org_obs, obs_params)\n",
        "            #task_state = task_state.replace(obs=normed_obs)\n",
        "\n",
        "            actions, policy_state = policy_net.get_actions(\n",
        "                task_state, params, policy_state)\n",
        "\n",
        "\n",
        "            if task.multi_agent_training:\n",
        "                task_state = task_state.replace(\n",
        "                    obs=task_state.obs.reshape(\n",
        "                        (num_tasks, num_agents, *task_state.obs.shape[1:])))\n",
        "                actions = actions.reshape(\n",
        "                    (num_tasks, num_agents, *actions.shape[1:]))\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    configs = parse_args()\n",
        "    if configs.gpu_id is not None:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = configs.gpu_id\n",
        "    main(configs)"
      ],
      "metadata": {
        "id": "VNhycxEAulEI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}